{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRAIRIES IMPORTATION\n",
    "###############################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Activation, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA IMPORTATION\n",
    "###############################################\n",
    "\n",
    "#Import the data from the data folder\n",
    "#in the data folder there is two folder one is Testing and another is Training\n",
    "#in each folder there is 4 other folder one is glioma_tumor,meningioma_tumor,no_tumor and pituitary_tumor\n",
    "#in each folder there is images of the tumor in a jpg format\n",
    "#we are going to import the data from the training folder\n",
    "#we are going to import the data from the testing folder\n",
    "\n",
    "CATEGORIES = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
    "TRAINING_DIR = \"H:/Desktop/Machine Learning/CS584-TumorsBrain/data/Training/\"\n",
    "TESTING_DIR = \"H:/Desktop/Machine Learning/CS584-TumorsBrain/data/Testing/\"\n",
    "IMG_SIZE = 256\n",
    "\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "\n",
    "#This function is going to read the image and convert it into a numpy array\n",
    "#and then resize the image to 256*256\n",
    "#and then append the image and the class number to the training_data list if the parameter train is true else append to the testing_data list\n",
    "def create_data(train = True):\n",
    "    for category in CATEGORIES:\n",
    "        if train:\n",
    "            path = os.path.join(TRAINING_DIR, category)\n",
    "        else:\n",
    "            path = os.path.join(TESTING_DIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                if train:\n",
    "                    training_data.append([new_array, class_num])\n",
    "                else:\n",
    "                    testing_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "#We are going to call the create_data function to create the training data and testing data\n",
    "create_data(train = True)\n",
    "create_data(train = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA VISUALIZATION\n",
    "###############################################\n",
    "\n",
    "#We are going to visualize the data\n",
    "#We are going to visualize the training data\n",
    "#We are going to visualize the testing data\n",
    "\n",
    "#Visualize the sample number of each class in the training data\n",
    "def visualize_data(train = True):\n",
    "    glioma_tumor = 0\n",
    "    meningioma_tumor = 0\n",
    "    no_tumor = 0\n",
    "    pituitary_tumor = 0\n",
    "    data_set = training_data if train else testing_data\n",
    "    for features, label in data_set:\n",
    "        if label == 0:\n",
    "            glioma_tumor += 1\n",
    "        elif label == 1:\n",
    "            meningioma_tumor += 1\n",
    "        elif label == 2:\n",
    "            no_tumor += 1\n",
    "        elif label == 3:\n",
    "            pituitary_tumor += 1\n",
    "    data_set_name = \"TRAINING\" if train else \"TESTING\"\n",
    "    print(data_set_name + \" DATA SET:\")\n",
    "    print(\"glioma_tumor: \", glioma_tumor)\n",
    "    print(\"meningioma_tumor: \", meningioma_tumor)\n",
    "    print(\"no_tumor: \", no_tumor)\n",
    "    print(\"pituitary_tumor: \", pituitary_tumor)\n",
    "    print(\"Total number of images: \", glioma_tumor + meningioma_tumor + no_tumor + pituitary_tumor)\n",
    "    plt.bar([\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"], [glioma_tumor, meningioma_tumor, no_tumor, pituitary_tumor])\n",
    "    if train:\n",
    "        plt.title(\"Training Data\")\n",
    "    else:\n",
    "        plt.title(\"Testing Data\")\n",
    "    plt.show()\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "#We are going to call the visualize_data function to visualize the training data and testing data\n",
    "visualize_data(train = True)\n",
    "visualize_data(train = False)\n",
    "\n",
    "\n",
    "#We will show one image from each class in the training data\n",
    "def show_image(train = True):\n",
    "    data_set = training_data if train else testing_data\n",
    "    data_set_name = \"TRAINING\" if train else \"TESTING\"\n",
    "    print(data_set_name + \" DATA SET:\")\n",
    "    #make a 4*4 grid and big size of each image\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    #glioma_tumor\n",
    "    for features, label in data_set:\n",
    "        if label == 0:\n",
    "            axs[0, 0].imshow(features, cmap = \"gray\")\n",
    "            axs[0, 0].set_title(\"glioma_tumor\")\n",
    "            break\n",
    "    #meningioma_tumor\n",
    "    for features, label in data_set:\n",
    "        if label == 1:\n",
    "            axs[0, 1].imshow(features, cmap = \"gray\")\n",
    "            axs[0, 1].set_title(\"meningioma_tumor\")\n",
    "            break\n",
    "    #no_tumor\n",
    "    for features, label in data_set:\n",
    "        if label == 2:\n",
    "            axs[1, 0].imshow(features, cmap = \"gray\")\n",
    "            axs[1, 0].set_title(\"no_tumor\")\n",
    "            break\n",
    "    #pituitary_tumor\n",
    "    for features, label in data_set:\n",
    "        if label == 3:\n",
    "            axs[1, 1].imshow(features, cmap = \"gray\")\n",
    "            axs[1, 1].set_title(\"pituitary_tumor\")\n",
    "            break\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#We are going to call the show_image function to show one image from each class in the training data and testing data\n",
    "show_image(train = True)\n",
    "show_image(train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PREPROCESSING\n",
    "###############################################\n",
    "\n",
    "#We are going to shuffle the training data and testing data\n",
    "#We are going to separate the features and the labels for the training data and testing data\n",
    "#We are going to split the training data into training data and validation data\n",
    "#We are going to normalize the training, validation and testing data and one-hot encode the labels\n",
    "\n",
    "############################################################################################################\n",
    "# - Shuffle the training data and testing data\n",
    "\n",
    "#We are going to shuffle the training data\n",
    "random.shuffle(training_data)\n",
    "#We are going to shuffle the testing data\n",
    "random.shuffle(testing_data)\n",
    "\n",
    "############################################################################################################\n",
    "# - Separate the features and the labels for the training data and testing data\n",
    "\n",
    "#We are going to separate the features and the labels for the training data\n",
    "x_train = []\n",
    "y_train = []\n",
    "for features, label in training_data:\n",
    "    x_train.append(features)\n",
    "    y_train.append(label)\n",
    "#We are going to separate the features and the labels for the testing data\n",
    "x_test = []\n",
    "y_test = []\n",
    "for features, label in testing_data:\n",
    "    x_test.append(features)\n",
    "    y_test.append(label)\n",
    "\n",
    "############################################################################################################\n",
    "# - Split the training data into training data and validation data\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "val_size = int(len(x_train) * VAL_RATIO)\n",
    "x_val = x_train[:val_size]\n",
    "y_val = y_train[:val_size]\n",
    "x_train = x_train[val_size:]\n",
    "y_train = y_train[val_size:]\n",
    "\n",
    "############################################################################################################\n",
    "# - Normalize the training, validation and testing data and one-hot encode the labels\n",
    "\n",
    "#We are going to normalize the training data by dividing it by 255\n",
    "x_train = np.array(x_train) / 255\n",
    "#We are going to normalize the validation data\n",
    "x_val = np.array(x_val) / 255\n",
    "#We are going to normalize the testing data\n",
    "x_test = np.array(x_test) / 255\n",
    "\n",
    "#We are going to convert the labels to one-hot vectors\n",
    "y_train = to_categorical(y_train, num_classes = 4)\n",
    "y_val = to_categorical(y_val, num_classes = 4)\n",
    "y_test = to_categorical(y_test, num_classes = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    epochs = {{choice([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])}}\n",
    "    #We are going to use hyperas to find the best batch size\n",
    "    batch_size = {{choice([16, 32, 64, 128, 256])}}\n",
    "    #We are going to use hyperas to find the best learning rate\n",
    "    learning_rate = {{uniform(0, 1)}}\n",
    "    #We are going to use hyperas to find the best optimizer\n",
    "    optimizer = {{choice(['adam', 'rmsprop', 'sgd'])}}\n",
    "    #We are going to use hyperas to find the best activation function\n",
    "    activation = {{choice(['relu', 'elu', 'selu', 'tanh', 'sigmoid'])}}\n",
    "    #We are going to use hyperas to find the best number of neurons in the hidden layer\n",
    "    neurons = {{choice([16, 32, 64, 128, 256, 512, 1024])}}\n",
    "    #We are going to use hyperas to find the best dropout rate\n",
    "    dropout_rate = {{uniform(0, 1)}}\n",
    "    #We are going to use hyperas to find the best number of layers\n",
    "    num_layers = {{choice([1, 2, 3, 4, 5])}}\n",
    "    #We are going to use hyperas to find the best number of neurons in each layer\n",
    "    neurons_1 = {{choice([16, 32, 64, 128, 256, 512, 1024])}}\n",
    "    neurons_2 = {{choice([16, 32, 64, 128, 256, 512, 1024])}}\n",
    "    neurons_3 = {{choice([16, 32, 64, 128, 256, 512, 1024])}}\n",
    "    neurons_4 = {{choice([16, 32, 64, 128, 256, 512, 1024])}}\n",
    "    neurons_5 = {{choice([16, 32, 64, 128, 256, 512, 1024])}}\n",
    "    #We are going to use hyperas to find the best batch normalization\n",
    "    batch_normalization = {{choice([True, False])}}\n",
    "    #We are going to use hyperas to find the best kernel initializer\n",
    "    kernel_initializer = {{choice(['glorot_uniform', 'he_uniform', 'lecun_uniform', 'glorot_normal', 'he_normal', 'lecun_normal'])}}\n",
    "    #We are going to use hyperas to find the best kernel regularizer\n",
    "    kernel_regularizer = {{choice(['l1', 'l2', 'l1_l2'])}}\n",
    "    #We are going to use hyperas to find the best bias regularizer\n",
    "    bias_regularizer = {{choice(['l1', 'l2', 'l1_l2'])}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST MODEL: A SIMPLE NEURAL NETWORK\n",
    "###############################################\n",
    "\n",
    "#We are going to build a simple neural network model\n",
    "#We are going to train the model\n",
    "#We are going to evaluate the model\n",
    "\n",
    "############################################################################################################\n",
    "# - Build a simple neural network model\n",
    "\n",
    "#We are going to build a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (IMG_SIZE, IMG_SIZE)))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(4, activation = \"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "############################################################################################################\n",
    "# - Train the model\n",
    "\n",
    "#We are going to compile the model\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0005), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "#We are going to train the model\n",
    "history = model.fit(x_train, y_train, epochs = 20, batch_size = 32, validation_data = (x_val, y_val))\n",
    "\n",
    "############################################################################################################\n",
    "# - Plot the training and validation accuracy and loss\n",
    "\n",
    "#We are going to plot the training and validation accuracy and loss\n",
    "def plot_accuracy_loss(history):\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label = \"Training Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label = \"Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label = \"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label = \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#We are going to call the plot_accuracy_loss function to plot the training and validation accuracy and loss\n",
    "plot_accuracy_loss(history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# - Train the model on the training data and validation data\n",
    "\n",
    "#We are going to concatenate the training data and validation data\n",
    "x_train_val = np.concatenate((x_train, x_val), axis = 0)\n",
    "y_train_val = np.concatenate((y_train, y_val), axis = 0)\n",
    "\n",
    "#We are going to train the model on the training data and validation data\n",
    "history = model.fit(x_train_val, y_train_val, epochs = 13, verbose = 0)\n",
    "\n",
    "############################################################################################################\n",
    "# - Evaluate the model on the testing data\n",
    "\n",
    "#We are going to evaluate the model on the testing data\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy: \" + str(test_accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# - Save the model\n",
    "\n",
    "#We are going to save the model\n",
    "model.save(\"model_ANN.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND MODEL: A CONVOLUTIONAL NEURAL NETWORK\n",
    "#####################################################\n",
    "\n",
    "#We are going to build a convolutional neural network model\n",
    "#We are going to train the model\n",
    "#We are going to evaluate the model\n",
    "\n",
    "############################################################################################################\n",
    "# - Build a convolutional neural network model\n",
    "\n",
    "#We are going to build a convolutional neural network model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\", input_shape = (IMG_SIZE, IMG_SIZE, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(512, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(4, activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# - Load the data using ImageDataGenerator\n",
    "\n",
    "CATEGORIES = [\"glioma_tumor\", \"meningioma_tumor\", \"no_tumor\", \"pituitary_tumor\"]\n",
    "TRAINING_DIR = \"H:/Desktop/Machine Learning/CS584-TumorsBrain/data/Training/\"\n",
    "TESTING_DIR = \"H:/Desktop/Machine Learning/CS584-TumorsBrain/data/Testing/\"\n",
    "IMG_SIZE = 256\n",
    "WITH_DATA_AUGMENTATION = False\n",
    "\n",
    "#We are going to load the data using ImageDataGenerator\n",
    "if WITH_DATA_AUGMENTATION:\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                        rotation_range = 40,\n",
    "                                        width_shift_range = 0.2,\n",
    "                                        height_shift_range = 0.2,\n",
    "                                        shear_range = 0.2,\n",
    "                                        zoom_range = 0.2,\n",
    "                                        horizontal_flip = True,\n",
    "                                        fill_mode = \"nearest\",\n",
    "                                        validation_split = 0.2)\n",
    "else:\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255, validation_split = 0.2)\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255, validation_split = 0.2)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "#We are going to load the training data\n",
    "training_set = train_datagen.flow_from_directory(TRAINING_DIR\n",
    "                                                , target_size = (IMG_SIZE, IMG_SIZE)\n",
    "                                                , batch_size = 32\n",
    "                                                , class_mode = \"categorical\"\n",
    "                                                , subset = \"training\"\n",
    "                                                , shuffle = True\n",
    "                                                , seed = 42)\n",
    "\n",
    "#We are going to load the validation data\n",
    "validation_set = val_datagen.flow_from_directory(TRAINING_DIR\n",
    "                                                , target_size = (IMG_SIZE, IMG_SIZE)\n",
    "                                                , batch_size = 32\n",
    "                                                , class_mode = \"categorical\"\n",
    "                                                , subset = \"validation\"\n",
    "                                                , shuffle = True\n",
    "                                                , seed = 42)\n",
    "\n",
    "#We are going to load the testing data\n",
    "testing_set = test_datagen.flow_from_directory(TESTING_DIR\n",
    "                                                , target_size = (IMG_SIZE, IMG_SIZE)\n",
    "                                                , batch_size = 32\n",
    "                                                , class_mode = \"categorical\"\n",
    "                                                , shuffle = False\n",
    "                                                , seed = 42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "# - Train the model\n",
    "\n",
    "#We are going to compile the model\n",
    "\n",
    "#We are going to use the Adam optimizer\n",
    "#We are going to use the categorical crossentropy loss function\n",
    "#We are going to use the accuracy metric\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0001), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "#We are going to train the model\n",
    "history = model.fit(training_set, epochs = 1, validation_data = validation_set)\n",
    "\n",
    "############################################################################################################\n",
    "# - Evaluate the model\n",
    "\n",
    "#Plot the training and validation accuracy and loss\n",
    "def plot_accuracy_loss(history):\n",
    "    plt.figure(figsize = (15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label = \"Training Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label = \"Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label = \"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label = \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "#We are going to evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(testing_set)\n",
    "print(\"Test Accuracy: \" + str(test_accuracy))\n",
    "\n",
    "#We are going to plot the training and validation accuracy and loss\n",
    "plot_accuracy_loss(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of the activations on a single image\n",
    "############################################################################################################\n",
    "# - Load the image\n",
    "\n",
    "#We are going to load the image and inmport the necessary libraries\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img = image.load_img(\"H:/Desktop/Machine Learning/CS584-TumorsBrain/data/Testing/glioma_tumor/image(1).jpg\", target_size = (IMG_SIZE, IMG_SIZE))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "############################################################################################################\n",
    "#Visualize the activations of the filters\n",
    "\n",
    "#We are going to visualize the activations of the filters\n",
    "from tensorflow.keras import models\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "activations = activation_model.predict(img)\n",
    "\n",
    "#We are going to plot the activations\n",
    "def plot_activations(activations, layer_names):\n",
    "    images_per_row = 16\n",
    "    for layer_name, layer_activation in zip(layer_names, activations):\n",
    "        n_features = layer_activation.shape[-1]\n",
    "        size = layer_activation.shape[1]\n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        for col in range(n_cols):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
    "                display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
    "        scale = 1. / size\n",
    "        plt.figure(figsize = (scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect = \"auto\", cmap = \"viridis\")\n",
    "\n",
    "#We are going to plot the activations\n",
    "layer_names = []\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "plot_activations(activations, layer_names)\n",
    "\n",
    "#We are going to plot the activations of the filters by initializing a random image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of the filters learned by the convolutional layers by finding the input image that maximizes the activation of a given filter\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#We load the model\n",
    "#model = load_model('cats_and_dogs_small_1.h5')\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "\n",
    "def deprocess_image(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_pattern(layer_number, filter_index, size=150):\n",
    "    layer_output = layer_outputs[layer_number]\n",
    "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "    #We compute the gradient of the loss with respect to the input\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "    iterate = K.function([model.input], [loss, grads])\n",
    "    #We initialize the input image with a random noise\n",
    "\n",
    "    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
    "\n",
    "    step = 0.8\n",
    "    #We run gradient ascent for 40 steps\n",
    "    for i in range(65):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "    img = input_img_data[0]\n",
    "    return deprocess_image(img)\n",
    "\n",
    "#We plot the first filter of the first convolutional layer\n",
    "\n",
    "plt.imshow(generate_pattern(5, 7))\n",
    "plt.show()\n",
    "\n",
    "#We plot all the filters of the first convolutional layer in black and white\n",
    "\n",
    "#We plot all the filters of each convolutional layer in black and white (the first convolutional layer has 32 filters, the second 64, the third 128, the fourth 128)\n",
    "\n",
    "print(\"GRID FOR THE FIRST CONVOLUTIONAL LAYER\")\n",
    "#make a grid of 8*4 images\n",
    "fig, ax = plt.subplots(8, 4, figsize=(32, 32))\n",
    "for i in range(8):\n",
    "    for j in range(4):\n",
    "        ax[i, j].imshow(generate_pattern(0, (i+1) * (1+j) - 1), cmap='viridis')\n",
    "        ax[i, j].axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"GRID FOR THE SECOND CONVOLUTIONAL LAYER\")\n",
    "#make a grid of 8*8 images\n",
    "fig, ax = plt.subplots(8, 8, figsize=(32, 32))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        ax[i, j].imshow(generate_pattern(2, (i+1) * (1+j) - 1), cmap='viridis')\n",
    "        ax[i, j].axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"GRID FOR THE THIRD CONVOLUTIONAL LAYER\")\n",
    "#make a grid of 16*8 images\n",
    "fig, ax = plt.subplots(16, 8, figsize=(32, 32))\n",
    "for i in range(16):\n",
    "    for j in range(8):\n",
    "        ax[i, j].imshow(generate_pattern(4, (i+1) * (1+j) - 1), cmap='viridis')\n",
    "        ax[i, j].axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"GRID FOR THE FOURTH CONVOLUTIONAL LAYER\")\n",
    "#make a grid of 16*8 images\n",
    "fig, ax = plt.subplots(16, 8, figsize=(32, 32))\n",
    "for i in range(16):\n",
    "    for j in range(8):\n",
    "        ax[i, j].imshow(generate_pattern(6, (i+1) * (1+j) - 1), cmap='viridis')\n",
    "        ax[i, j].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
